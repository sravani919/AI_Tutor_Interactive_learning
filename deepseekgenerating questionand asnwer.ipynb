{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7631f8cb-49bc-4cbf-8cf8-1701eace0572",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import json\n",
    "import time\n",
    "import fitz  # PyMuPDF for PDF extraction\n",
    "\n",
    "# ✅ Load DeepSeek Model\n",
    "model_name = \"deepseek-ai/deepseek-coder-6.7b-instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, \n",
    "    torch_dtype=torch.float16, \n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# ✅ Function to Extract Text from PDF\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"Extracts all text from the PDF file.\"\"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text = \"\"\n",
    "    for page in doc:\n",
    "        text += page.get_text(\"text\")\n",
    "    return text\n",
    "\n",
    "# ✅ Function to Generate Questions with Time Limit & Better Control\n",
    "def generate_questions_deepseek(chapter_title, chapter_content):\n",
    "    \"\"\"Generate review questions using DeepSeek AI model with improved token control.\"\"\"\n",
    "    \n",
    "    # Limit chapter content to prevent slow execution\n",
    "    truncated_content = chapter_content[:700]  # Keeping it smaller for speed\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    Based on the following textbook chapter, generate 5 review questions.\n",
    "\n",
    "    Chapter Title: {chapter_title}\n",
    "    Chapter Content:\n",
    "    {truncated_content}\n",
    "\n",
    "    Questions:\n",
    "    \"\"\"\n",
    "\n",
    "    # ✅ Tokenize & Limit Tokens to Avoid Freezing\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512).to(\"cuda\")\n",
    "    \n",
    "    try:\n",
    "        # ✅ Run Generation with Timeout (Prevents Hanging)\n",
    "        start_time = time.time()\n",
    "        outputs = model.generate(\n",
    "            **inputs, \n",
    "            max_new_tokens=250,  # Slightly increased to ensure full questions\n",
    "            pad_token_id=tokenizer.eos_token_id,  # Ensures stopping\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "        \n",
    "        # Stop if it takes more than 30 seconds per call\n",
    "        if time.time() - start_time > 30:\n",
    "            print(f\"⚠️ Timeout: Skipping {chapter_title} due to long execution time.\")\n",
    "            return [\"Timeout: No questions generated.\"]\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Error while generating questions for {chapter_title}: {str(e)}\")\n",
    "        return [\"Error occurred during generation.\"]\n",
    "\n",
    "    # ✅ Decode Response\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    return generated_text.strip().split(\"\\n\")\n",
    "\n",
    "# ✅ Function to Generate Summaries with Time Limit & Better Control\n",
    "def generate_summary_deepseek(chapter_title, chapter_content):\n",
    "    \"\"\"Generate a summary for a chapter using DeepSeek AI model with improved efficiency.\"\"\"\n",
    "    \n",
    "    # ✅ Reduce text size to avoid infinite generation\n",
    "    truncated_content = chapter_content[:900]  # Reduced for speed\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    Summarize the following textbook chapter in 3-5 sentences:\n",
    "\n",
    "    Chapter Title: {chapter_title}\n",
    "    Chapter Content:\n",
    "    {truncated_content}\n",
    "\n",
    "    Summary:\n",
    "    \"\"\"\n",
    "\n",
    "    # ✅ Tokenize & Limit Tokens\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512).to(\"cuda\")\n",
    "    \n",
    "    try:\n",
    "        # ✅ Run Generation with Timeout\n",
    "        start_time = time.time()\n",
    "        outputs = model.generate(\n",
    "            **inputs, \n",
    "            max_new_tokens=150,  # Keep short summaries\n",
    "            pad_token_id=tokenizer.eos_token_id, \n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "        \n",
    "        # Stop if it takes more than 20 seconds\n",
    "        if time.time() - start_time > 20:\n",
    "            print(f\"⚠️ Timeout: Skipping {chapter_title} due to long execution time.\")\n",
    "            return \"Timeout: No summary generated.\"\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Error while generating summary for {chapter_title}: {str(e)}\")\n",
    "        return \"Error occurred during summary generation.\"\n",
    "\n",
    "    # ✅ Decode Response\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    return generated_text.strip()\n",
    "\n",
    "# ✅ Load and Extract Text from PDF\n",
    "pdf_path = \"Workplace_Software_and_Skills_-_WEB_IlfJtcP.pdf\"\n",
    "textbook_text = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "# ✅ Split the extracted text into chapters (Assuming each chapter is separated by a recognizable pattern)\n",
    "# Here, I am assuming that chapters are separated by \"Chapter\" and a number. You may need to adjust this logic based on your PDF structure.\n",
    "chapters = textbook_text.split(\"Chapter\")  # Adjust based on how chapters are defined in your text\n",
    "chapter_titles = []\n",
    "chapter_contents = []\n",
    "\n",
    "for chapter in chapters[1:]:  # Skip the first empty entry if it exists\n",
    "    lines = chapter.split(\"\\n\", 1)\n",
    "    chapter_titles.append(\"Chapter \" + lines[0].strip())\n",
    "    chapter_contents.append(lines[1].strip() if len(lines) > 1 else \"\")\n",
    "\n",
    "# ✅ Generate Questions and Summaries for All Chapters\n",
    "review_questions = {}\n",
    "chapter_summaries = {}\n",
    "\n",
    "for chapter_title, chapter_content in zip(chapter_titles, chapter_contents):\n",
    "    # Generate questions for each chapter\n",
    "    questions = generate_questions_deepseek(chapter_title, chapter_content)\n",
    "    review_questions[chapter_title] = questions\n",
    "    \n",
    "    # Generate summary for each chapter\n",
    "    summary = generate_summary_deepseek(chapter_title, chapter_content)\n",
    "    chapter_summaries[chapter_title] = summary\n",
    "\n",
    "# ✅ Save Generated Questions and Summaries as JSON\n",
    "questions_json_path = \"deepseek_review_questions.json\"\n",
    "summaries_json_path = \"deepseek_chapter_summaries.json\"\n",
    "\n",
    "with open(questions_json_path, \"w\", encoding=\"utf-8\") as json_file:\n",
    "    json.dump(review_questions, json_file, indent=4)\n",
    "\n",
    "with open(summaries_json_path, \"w\", encoding=\"utf-8\") as json_file:\n",
    "    json.dump(chapter_summaries, json_file, indent=4)\n",
    "\n",
    "print(f\"✅ Review questions generated and saved to {questions_json_path}\")\n",
    "print(f\"✅ Chapter summaries generated and saved to {summaries_json_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
